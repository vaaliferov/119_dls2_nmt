* [Темы проектов](https://docs.google.com/spreadsheets/d/1vn8duS-70Rw4-OZwj7ZZrZ72vFVBAV8d1r2LXZTopYE/edit)
* [Выбор проектов](https://docs.google.com/forms/d/e/1FAIpQLSfWqwFoeUaVM6eklfw88u4QrMXCnIBETBJjbmMj9f4ycdrgiw/viewform)
* [Информация о проектах](https://docs.google.com/document/d/192JgT4c1qjGXNbng6DPCGdsFnqArOj8eCxEyIG4tpnU/edit)
* [NMT project (slack)](https://app.slack.com/client/T01AR4TAMN0/C0249M57N2E/thread/C0249M57N2E-1624646978.004600)
* [Описание задачи](https://docs.google.com/document/d/1Hqg7ftVDlwqlw8Ew7hebZt3nzfmFT-0h6ipGOLvK7os/edit)
* [The Story of Heads](https://lena-voita.github.io/posts/acl19_heads.html)
* [The Story of Heads (github)](https://github.com/lena-voita/the-story-of-heads)
* [Model Compression and Acceleration (1:28:00)](https://disk.yandex.ru/i/Ihdr-t9tYgsaMA)
* [YSDA Natural Language Processing course](https://github.com/yandexdataschool/nlp_course)
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [The Narrated Transformer Language Model](https://www.youtube.com/watch?v=-QH8fRhqFHM)
* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
[:link:](https://github.com/harvardnlp/annotated-transformer)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Analyzing Multi-Head Self-Attention](https://aclanthology.org/P19-1580.pdf)
* [Learning Sparse Neural Networks through L0 Regularization](https://openreview.net/pdf?id=H1Y8hhg0b)
* [On Pixel-Wise Explanations for Non-Linear Classifier Decisions](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0130140&type=printable)
* [Analyzing the Source and Target Contributions to Predictions](https://arxiv.org/pdf/2010.10907.pdf)
* [Pruning Attention Heads of Transformer Models Using A* Search](https://arxiv.org/abs/2110.15225)
* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
* [Generating Multilingual Parallel Corpus Using Subtitles](https://arxiv.org/abs/1804.03923)
* [Extracting Large Parallel Corpora from Movie and TV Subtitles](https://aclanthology.org/L16-1147.pdf)
* [Finding the Optimal Vocabulary Size for Neural Machine Translation](https://aclanthology.org/2020.findings-emnlp.352.pdf)
* [Understanding the difficulty of training.. (Xavier Glorot)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
* [Understanding the Difficulty of Training Transformers](https://arxiv.org/pdf/2004.08249.pdf)
* [Sensitivity Guided Adaptive Learning Rate](https://openreview.net/pdf?id=cuvga_CiVND)
* [Improving Transformer Optimization Through Better Initialization](https://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf)
* [Advanced Techniques for Fine-tuning Transformers](https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e)
* [Huggingface Optimization Module](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules)
* [How to decay your learning rate](https://arxiv.org/pdf/2103.12682.pdf)
* [**Calling loss.item() is very slow**](https://discuss.pytorch.org/t/calling-loss-item-is-very-slow/99774)
----
* [Ben Trevett Seq2Seq](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)
* [Samsung Transformer](https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task5_text_transformer.ipynb)
* [Nesty Me Transformer](https://drive.google.com/drive/folders/1qXkPrDyjH3fp0ufkvUspqPQllCJRt7NM?usp=sharing)
* [IlyaStepanov Pruning](https://github.com/IlyaStepanov/DLS_project_part2_Transformer)
----
* https://opus.nlpl.eu/Books.php
* https://opus.nlpl.eu/OpenSubtitles-v2018.php
* https://huggingface.co/datasets/open_subtitles
* https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/moses/en-ru.txt.zip
----
* https://github.com/pytorch/pytorch/blob/master/torch/nn/init.py#L288
* https://github.com/pytorch/pytorch/blob/master/torch/nn/init.py#L304
* https://github.com/lena-voita/the-story-of-heads/blob/master/lib/layers/basic.py
* https://github.com/lena-voita/the-story-of-heads/blob/master/lib/layers/attn.py
* https://github.com/lena-voita/the-story-of-heads/blob/master/lib/layers/attn.py#L206
* https://github.com/lena-voita/the-story-of-heads/blob/master/lib/layers/concrete_gate.py
* https://github.com/lena-voita/the-story-of-heads/blob/master/scripts/train_fixed_alive_heads.sh
* https://github.com/lena-voita/the-story-of-heads/blob/efaa0dd520400baa760654b5b85396c203d3cbb7/lib/layers/attn.py#L272

--------------

* https://opus.nlpl.eu/OpenSubtitles/ru&en/v2018/OpenSubtitles

* https://www.kaggle.com
* https://colab.research.google.com

* https://github.com/bentrevett/pytorch-seq2seq
* http://jalammar.github.io/illustrated-transformer
* https://github.com/harvardnlp/annotated-transformer

* https://github.com/lena-voita/the-story-of-heads
* https://lena-voita.github.io/posts/acl19_heads.html

* https://github.com/tqdm/tqdm
* https://github.com/numpy/numpy
* https://github.com/pytorch/text
* https://github.com/wkentaro/gdown
* https://github.com/pytorch/pytorch
* https://github.com/plotly/plotly.py
* https://github.com/VKCOM/YouTokenToMe
* https://github.com/matplotlib/matplotlib
* https://github.com/tdlib/telegram-bot-api
* https://github.com/python-telegram-bot/python-telegram-bot
