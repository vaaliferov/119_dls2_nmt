* 0.0 ack aspic
* 1.x model architecture
* 2.1 source tokens histogram
* 2.2 target tokens histogram
* 3.1 learning rate plot (noam schedule)
* 3.2 learning rate plot (cosine schedule)
* 3.3 learning rate plot (cosine schedule, pruning)
* 4.1 attention gates (animation)
* 4.2 attention gates (init state)
* 4.3 attention gates (final state)
* 5.1 encoder self-attention (6 layers * 8 heads)
* 5.2 decoder self-attention (6 layers * 8 heads)
* 5.3 decoder-encoder attention (6 layers * 8 heads)
* 6.1 loss & bleu plot (train: epochs 3-10)
* 6.2 loss & bleu plot (train: chunks on)
* 6.3 loss & bleu plot (valid: chunks off)
* 7.1 loss & bleu plot (prun: λ = 2.00, β = 0.50, 70k iterations)
* 7.2 loss & bleu plot (prun: λ = 1.00, β = 0.33, 70k iterations)
* 7.3 loss & bleu plot (prun: λ = 0.50, β = 0.50, 70k iterations)
* 7.4 loss & bleu plot (prun: λ = 0.05, β = 0.50, 70k iterations)
* 7.5 loss & bleu plot (prun: λ = 0.05, β = 0.50, 215k iterations)
* 8.1 loss & bleu plot (train: 1-10 epochs)
* 8.2 loss & bleu plot (valid: 1-10 epochs)
* 9.x screenshots (notify & nmt bots)